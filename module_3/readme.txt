Name: Youngmin Park (ypark79)

Module Info: Module 3 Assignment: Database Queries Assignment Experiment . Due on 08 FEB 2026 at 11:59 EST. 

SSH URL to GitHub Repo: git@github.com:ypark79/jhu_software_concepts.git
Link to Module 3 Folder in GitHub Repo: https://github.com/ypark79/jhu_software_concepts/tree/main/module_3


Overview – 
This readme will explain my approach for the two portions of this programming assignment: 1) Creating the postgreSQL data base, loading the 30,000 student entries scraped and cleaned in module 2, and answering a series of queries. 2) Programming the frontend and backend of a Flask Webpage designed to scrape/clean recent additions to the gradcafe website, analyze the new data based on the same queries from Part 1, and display the results on a dynamic Flask webpage. Of note, part 2 of this assignment required modification of the code from my mod_2 scraper/cleaner program. In order to run my module_3 assignment, create a postgreSQL database, run db_connection to connect to it, run load_data.py to create the table and insert data, run app.py, open the web browser, and click the "Pull Data" button to trigger the module 2 program. 

Module 3 Folder Infrastructure: 
The files for this assignment are located in the module_3 folder (both on my local repo and on my GitHub). Inside module_3 are db_connection.py, load_data.py, query_data.py, and app.py. Within module_3 are three subfolders: Data, Scraper, and templates. The Data folder has my original 30,000 entry pull from module 2 and is what load_data.py utilizes to insert data into my PostgreSQL database table. The Scraper folder includes all of module 2 code, which was updated to fulfill the requirements of the module 3 assignment. Lastly, templates holds index.html, which is the frontend of my single Flask webpage. All of my module 3 code properly maps to the file infrastructure listed above. 

Approach - 
I will now explain the general approach to this assignment before doing a deep dive on both portions of the assignment. After installing PostgreSQL utilizing Homebrew on my MacOS, I created a PostgreSQL database (titled module_3) on my local machine through my PyCharm terminal. Then I coded python files to create a connection to my database (db_connection.py), create a single applicant table and load the original 30,000 entries pulled/cleaned in module_2 (load_data.py), and answer a series of 9 prescribed questions plus 2 self-generated questions through SQL (query_data.py). I then provided a screenshot of the outputs of query_data.py based on my initial 30,000 entry pull as well as an explanation of what SQL code I utilized and why I used them for the questions. 

For the second part of the assignment, I modified my scrape.py and clean.py from module 2 in several ways to support my dynamic flask webpage, which pulls new entries from gradcafe and updates its answers to the 11 queries from part 1. Of note, the newly scraped/cleaned data from gradcafe was appended to the existing 30,000 entries from module 2 in order to increase the accuracy of the answers of the SQL queries thanks to a larger data pool. 

For scrape.py, I eliminated all intermediate JSON outputs for the raw scraped data since these were designed to help me troubleshoot during a large data pull. Scrape.py was also modified to only pull new entries off of gradcafe, which was done by coding a stopping mechanism once the code recognizes a unique result_id that already exists in the original 30,000 pull JSON file. Of note, for real world use, I would code scrape.py to scrape new entries until it identifies a unique result_id that already exists in the JSON output file. However, for my submission, I limited scrape.py to stop at 1000 new entries per scrape for two reasons: 1) For the purposes of grading. The longer the flask webpage is not utilized, the larger the new scrape will be, which could potentially require an overnight scrape/clean process. 2) I implemented this limit with future users in mind under the assumption that a user may not be willing to wait multiple hours or overnight to see new analysis from my webpage. I also added code to scrape.py to infer the term and year for every student application. Due to my initial 30,000 JSON pull failing to scrape the term and year for the applications during module 2, I debugged and fixed the code in scrape.py to ensure that it pulls the term data – this was missed in my module_2 submission. Of note, because I populated my PostgreSQL database with my original 30,000 pull from module 2, only new entries will find the term and append it to the JSON file. As I continue to run iterations of my program/webpage, the term field will populate correctly and eventually, the analysis will be accurate as more entries are pulled from gradcafe. 

Once scrape.py was successfully modified, I then modified my PostgreSQL database through my PyCharm terminal. I added an additional column for unique result_id in the PostgreSQL database applicant table. This was critical in ensuring that scrape.py only scrapes for new entries on gradcafe; I utilized the unique result_id at the end of each student application URL as a mechanism to determine whether entries were new or not.  It was critical to ensure the unique result_id column was added to the database so the Flask webpage pulls and analyzes the newly added data. 

Then I modified the code in clean.py to reflect all the changes in scrape.py. Additionally, code was added to append the newly scraped data into the currently existing “llm_extend_applicant_data.json” file. This was done because I did not want clean.py to overwrite the original 30,000 JSON pull; I wanted the new data to be appended to the 30,000 entry file to provide the user a more accurate analysis based on additional data. Lastly, code was added to clean.py to automatically update the PostgreSQL applicant database with the newly acquired and cleaned data from gradcafe. 

Once clean.py was updated, the Flask webpage backend was coded (app.py). The routes coded in app.py connects to my PostgreSQL applicant database, executes the 11 queries, retrieves collected data, and then pushes the data to be visually depicted on my webpage, which is the front end (index.html). I coded two buttons in my front end (index.html): one to pull new data and one to analyze that data. When the button to pull new data is selected, app.py triggers my scrape/clean code from module 2 to run by triggering main.py. Main.py triggers the local llm app.py to run, starts scrape.py, and then clean.py. Clean.py then automatically pushes the newly acquired cleaned data to my PostgreSQL database. Of note, this data is appended to my original 30,000 JSON file to ensure a larger datapool for the analysis. 

Lastly, I coded the frontend of the Flask webpage(index.html) utilizing in-line CSS  to stylize the webpage presentation, which included fonts, spacing, location of the buttons, the buttons themselves, and the highlighting of the buttons when they become available to select based on the pipeline. Index.html displays the analytics generated from the queries and does not require the user to interact with the database or SQL at all. The pull data button triggers the scraping and cleaning pipeline, which collects new entries from gradcafe, updates the database, and the updated SQL query results are displayed when the user refreshes the analysis. 

One important note regarding the JSON file "llm_extend_applicant_data.JSON." It currently exists in two locations: 1) module_3 - Data folder and 2) module_3 - Scraper Folder. When doing this assignment, I completed the first portion of the assignment first, which was to code load_data.py to build the applicant table and load the original 30,000 file entry into the postgreSQL database table. This llm_extend_applicant_data.json file is the one that exists in the module_3 - Data Folder. I left it there for grading purposes and for the instructor to be able to run my code to create a postgreSQL database and load the initial 30,000 pull from scratch. For the second portion of the assignment (programming the Flask Dynamic Webpage), my backend code (app.py) utilizes subprocess to ensure that main.py (initiator code for module 2 code) works in the working director of module_3 - Scraper folder since that is where my module_2 code exists. Therefore, when main.py initiates the scrape and clean pipeline for new entries, clean.py will append the new entries to the "llm_extend_applicant_data.json" file that exists inside module_3 - Scraper Folder. This is essentially the master JSON file. Then app.py will run load_data.py, which in this instance, will load the updated "llm_extend_applicant_data.json" file from the scraper folder since this is the current working directory. The only time load_data.py loads the JSON file in the module_3 - Data folder (original 30,000 files) is if load_data.py is ran by itself. 

See below for a deep dive into the key sections of my module 3 assignment. 


PostgreSQL Database– 
The first step to this assignment was to create a PostgreSQL database on my local machine. The following directions will be for creating a PostgreSQL database utilizing a MacOS. First, I had to ensure that homebrew was installed on my machine; I googled the Terminal Command to install and verify that I had the most up to date version of Homebrew. Then I installed postgreSQL utilizing Homebrew; this was done by using the simple command “brew install postgreSQL” in my terminal. I also ensured the PostgreSQL server starts up and runs in the background on my computer by using the command “brew services start postgreSQL.” I then created a PostgreSQL database by connecting to the PostgreSQL prompt using “psql postgres” and “CREATE DATABASE module_3” in my terminal. 


db_connection.py - 
Once my PostgreSQL database was created on my local machine, I then coded db_connection.py, who’s sole purpose is to create the connection to my database. I created a separate python file for the database connection because my flask webpage needs to be dynamic and needs to be able to connect to the database to pull data and add new data on its own. This is reflected in my app.py code.  Psycopg was utilized to open a connection to the PostgreSQL database. “except Exception as e” code block was used to account for a variety of errors such as PostgreSQL not running or incorrect credentials.


load_data.py – 
Next step was to code load_data.py, who’s primary function is to load the JSON file output from the Module_2 assignment (30,000 entries scraped and cleaned from gradcafe.). First step is to create a single table of applicant data as per the assignment instructions. This was accomplished by using the CREATE TABLE SQL function. Once the table was created, I then inserted the 30,000 entries that scraped and cleaned in the Module_2 assignment. I specifically utilized the “llm_extend_applicant_data.json” because it includes the llm-generated program and universities. As mentioned earlier, my scrape.py failed to properly collect term data. Although I fixed my scrape.py to properly scrape term data while working through module 3, I inserted code in load_data.py to infer the term and year from the data that was available from my original 30,000 pull – this is reflected in “def infer_term()”. I coded data type conversions in load_data.py to ensure that the date strings from the JSON file are converted to python date objects so that psycopg could properly insert them into the PostgreSQL database. This is reflected in “def parse_date() and def try_float(). Of note, for grading purposes, to assist the grader, I coded TRUNCATE TABLE and included my original 30,000 JSON file “llm_extend_applicant_data.json” . TRUNCATE TABLE clears any data existing in a preexisting table – this way, the grader can run my code from scratch using the provided 30,000 entry JSON file to observe that load_data.py works properly. If I were continuously running my project on my local machine, I would delete the TRUNCATE TABLE code. This also is a safety measure to ensure that my database matches my updated JSON file of entries. Lastly, I added code to ensure commit/rollback protections to avoid partially loaded tables and maintain consistency in my database. Lastly, as mentioned before, if load_data.py is ran independently, it will load the JSON file that is located in the module_3 - Data folder (this replicates the first part of this assignment). For the second part of the assignment, when app.py (backend) runs the Flask Webpage, load_data.py will load the JSON file located in module_3 - Scraper folder since this JSON file is the "master" file that has the original 30,000 plus any newly scraped/cleaned entries. load_data.py executing in these two different working directories was accomplished through the use of subprocess in app.py. 
 

query_data.py – 
The next step was to code query_data.py, which consists of SQL queries that answer the 9 prescribed questions and the 2 self-generated questions as per the assignment instructions. In addition to using SQL keywords and functions to answer the programming assignment questions, I also used ChatGPT to learn SQL keywords that help account for variations of the table fields (i.e. differing spacings between words, misspellings, targeted strings appearing in sentences, and the use of acronyms). This was largely accomplished by utilizing “ILIKE” SQL operator and the “%” wildcard character. The “ILIKE” SQL operator is used for case-insensitive pattern matching, which accounted for instances in which all letters or the first letter of a field were capitalized. The “%” wildcard character used in conjunction of the ILIKE operator accounted for times when the specific string I was looking for appeared in a sentence – the combination essentially conducts case-insensitive pattern matching while searching for substrings anywhere within text. ChatGPT also helped me learn SQL code that assisted in formatting the data imports. Examples include ROUND and ::DECIMAL, which allowed me to standardize fields that contained floats to be rounded to two decimal places. Additionally, ::DECIMAL prevents ROUND from turning small decimals into zero, which ultimately prevents “dividing by zero errors.”  ChatGPT assisted in learning and implementing the SQL functions COUNT, AVG, FILTER, ROUND, NULLIF, ILIKE, LIKE, and %. 


app.py – 
After completing the programming assignment portion of the Module_3 assignment, the next step was the Flask Webpage portion of the assignment. I programmed the app.py file to serve as the “backend” of my Flask Webpage. App.py reads data from my PostgreSQL applicant table, runs all the SQL queries as per the prescribed questions in the assignment, pushes the results to my front end (index.html) for display, and facilitates the “pull” button on my webpage to trigger the scraper/cleaner pipeline. @app.route(‘/’) connects to Postgres, runs all of the coded SQL queries, and renders the results to index.html. The “is scraping = scraping_process” code block disables the buttons on my webpage while the scraper/cleaner program is running as per the assignment instructions. @app.route(‘/pull-data’) starts the scraper/cleaner pipeline when the “pull” button is selected. Of note, I put all of my module 2 code in a subfolder named “Scraper” which is reflected in the code cwd = “Scraper.” As mentioned, before, this use of subprocess sets the working directory to be module 3_scraper so that clean.py updates the "llm_extend_applicant_data.json" file in the scraper folder and load_data.py uploads the postgreSQL database table with the master JSON file. The “sys.executable” code ensures that the correct Python interpreter is utilized as well. @app.route(‘/update-analysis’) refreshes the webpage to reflect the new analysis (aka answers to the queries based on the additional data from new entries). Of note, app.py runs on port 8080, while the module 2 scrape/clean program app.py runs on port 8000; it is critical to ensure they run on different ports to avoid errors. Lastly, @app.route(scrape_status) was added to determine if the scrape-clean process is still on going so that the front end (index.html) knows to keep the pull data and analyze data buttons disabled. 


index.html – 
After completing the backend of the Flask Webpage (app.py), I then coded the front end (index.html); ChatGPT assisted me in utilizing inline CSS and Jinja to stylize the Flask webpage. Index.html displays live query results from my PostgreSQL database table, which originates from app.py, and has two buttons: Pull Data and Update Analysis, along with user-friendly descriptions of what the buttons do as per the assignment instructions. The <form action> blocks are for the two buttons. The Update Analysis button calls the Flask route in app.py update_analysis and the Pull Data button calls the Flask route “pull_data”. Additionally, ChatGPT assisted in coding the <div class = flash messages> code blocks so that the user can see status updates that I coded in flash() in app.py. index.html calls on app.scrape-status to see if the scrape/clean process is ongoing. If it is, index.html will keep both pull and analyze buttons disabled with text informing the user that the pipeline is still executing.  


Known Bugs – 
This code runs with no issues. The only known issue overall is that my scrape.py file was debugged and fixed to correctly scrape gradcafe entries for the “term” field, which it failed to do properly in the module 2 submission. As a result, the initial 30,000 entry that load_data.py inserted into my PostgreSQL is missing a large volume of term data. In addition to fully corrected this issue in my scrape.py data, I also inserted code that inferred the term and year based on the data available in the original 30,000 data pool. The mitigating measures for this issue is that every pull from now will include accurate term and year data from gradcafe and will eventually provide accurate analysis involve the term. 


Citations – 
ChatGPT assisted with the learning and coding of this project. 
