Name: Youngmin Park (ypark79)

Module Info: Module 2 Assignment: Web Scraping. Due on 01 FEB 2026 at 11:59 EST. 

SSH URL to GitHub Repo: git@github.com:ypark79/jhu_software_concepts.git
Link to Module 2 Folder in GitHub Repo: https://github.com/ypark79/jhu_software_concepts/tree/main/module_2


Approach - 

Overall Structure: This data scraper / cleaner program consists for 4 code files: main.py, scrape.py, clean.py, and app.py. The files must be executed in a specific order; this is accomplished through main.py, which is coded so the user simply executes main.py to run the entirety of the program. Scrape.py opens all required webpages to scrape and extracts rows of data that contain all required data fields and writes it to a JSON file titled “raw_scraped_data.json. clean.py loads this JSON file of uncleaned data, extracts the desired data field text, cleans it to standardize formatting, pushes the raw Program and University data through the instructor-provided LLM (app.py), and merges all cleaned data to produce two JSON files: llm_extend_applicant_data.json and applicant_data.json. llm_extend_applicant_data.json contains all cleaned data fields in addition to the Program and University data fields cleaned by the LLM. Applicant_data.json is the final cleaned data set with all required data fields, which have been cleaned, formatted, and standardized in a dictionary format as prescribed in the assignment instructions. Lastly, app.py is the instructor-provided local LLM that takes in the raw Program and University data and cleans it. This readme will provide the approach of this code’s design by explaining the key pieces of all 4 python files and will document where ChatGPT was used to enhance code. 

Safety Measures and Guards: There were numerous issues throughout the process of coding this program; largely issues stemming from high volume of GET, POST, and response requests to/from the webpage server. ChatGPT also assisted in identifying other potential issues and assisted in coding various safety measures to guard against these instances. Examples include potential unidentifiable characters, pages with no data, server timeouts, data fields with no data, and program/server crashes. Safety/guard mechanisms include batching data into chunks to maximize volume of data transmission without overwhelming a server, retry mechanisms with set or exponentially increasing sleep/wait times, and saving/writing “checkpoint” JSON files at predesignated time intervals in the case of sudden program/sever crashes. 

Cleaning: clean.py will implement various REGEX methods and string indexing/slicing to isolate desired text fields, remove unnecessary white space, standardize spacing, standardizing formatting for alphabetical and numerical texts data fields. These methods are used to extract data field text from the rows of raw data produced by scrape.py. Of note, approximately half of the desired data fields were extracted from the “Notes” section, which required the code to navigate to and scrape from the URL in each student’s application. ChatGPT assisted in coding the various methods to effectively extract the desire data field text in a manner that would minimize issues while parsing. 


scrape.py – 
URLs: The base domain URL of the gradcafe website and the survey page were set to separate variables to allow the program to navigate to different pages, which will be required when scraping 30,000 student entries and their respective links to their student applications. 

def _make_request(): This function sends the GET request to the website to request data and includes several measures to address potential issues stemming from the website blocking a high volume of requests and potential suspicion of a scripted bot requesting and pulling a large volume of data. ChatGPT assisted with the code that makes the GET request look like its coming from a MAC user utilizing a popular standard web browser. Additionally, ChatGPT assisted with coding a safety guard that reduces the risk of losing connection with the website utilizing “Connection”: “keep-alive”. 

def download_html(): This function accepts the URL as a string and calls the _make_request() function to send a standardized GET request to download the HTML in bytes and decode into utf-8. ChatGPT assisted with coding a retry mechanism to make five attempts, guard against potential non-utf-8 characters that may crash the program, and a guard against potential HTTP errors with exponentially increasing wait times (utilizing “wait = 2**”) before retrying to maximize probability of a successful HTML download. The try and except code approach was used for this and error codes were set to the variable “e” inside of an “if” code block construct to address different potential error codes. Print functions were integrated to provide status updates and error identification to help troubleshoot when code crashes. Finally, “raise” was utilized at the end of this code to bypass errors that are unsolvable. 

Def extract_text(): This function takes the decoded HTML and converts it into a beautifulsoup object to facilitate parsing. 

Def extract_tr_rows_from_html(): This function calls def extract_text to extract all data inside <tr> and <td> tags from the beautifulsoup object. After observing the html text, the majority of the desired fields exist within these tags. The beautifulsoup method “find_all” was used to do this and only <tr><td> rows that contained data were appended to the empty list object. 

Def_row_dict_from_tr(): This function extracts all data within <td> tags and formats the data into dictionaries to facilitate consolidation and parsing. The “get_text” method was utilized to extract desired text from the data. Additionally, “.find” was used to find all <a> tags with “href” attributes to target and extract the links to all student applications. The full URL to each students’ application were constructed utilizing the website’s base domain URL and the extracted endpoints to each student application. Lastly, raw key-value pairs were generated utilizing the raw data fields. The data inside each student application URL contains approximately half of the desired fields. The “results_text_raw” key was left empty for the time being, the data within the application URLs will be extracted, inserted as a value to the “result_text_raw” key, and parsed later in the code.  

Def scrape_data(): This is the primary data scraper function in scrape.py and returns a complete list object of all consolidated rows of raw data. Once the raw rows of data were consolidated in the extracted list, the raw data within the student applications was extracted, added as a value to the “results_text_raw” key, and appended to the “extracted” list to joint the previously consolidated rows of raw data. ChatGPT assisted in coding a guard  against failed/empty link. 

Def save_data(): This function converts the raw data into JSON and writes it to a file. 

Def load-data(): This function reads a JSON file and will be used to read the raw dataset JSON file. ChatGPT suggested writing the raw data set at specified checkpoints in the case of a code/website crash; def load_data is utilized to open up these partial data sets so that the program does not have to restart scraping from zero. It also includes a safety measure in the case a JSON file cannot be loaded due to not existing; this prevents the whole code from crashing in this instance. 

If__name__ == “__Main_”: As per the assignment instructions, the code sets a 30,000 entry target. As mentioned in def load_data(), a checkpoint file variable was established in the case of code/server crashes as well as a variable for the final JSON file that will contain the fully cleaned dataset. A safety checkpoint was coded to save and print the raw dataset every 10 pages in case the code/web server crashes. The actual safety measure for restarting the scraping process at a checkpoint was then coded by calling the load_data() function, which takes the most recent checkpoint file that was saved. Additionally, the code estimates where the program left off by counting the total number of rows that were extracted and dividing that number by 50 (estimating approximately 50 entries per page). A counter was established and set to 0 before initiating the overarching while loop that will cycle until 30,000 entries of raw datasets are consolidated. An if statement was used to signal if the code is pulling from the first page or subsequent pages. The “page_url” variable is one of the arguments that go into the scrape_data function; it will cycle the scraper through all pages to scrape the raw dataset rows. The scrape_data() function is then called to initiate the data scraping process. Print functions with status updates are integrated throughout to confirm that the scraping is ongoing. When exploring potential issues that could arise with ChatGPT, the LLM assisted with the coding of a safety mechanism in the case of empty pages utilizing an if statement and adding to the counter variable for empty pages; the assumption here is that if there are 5 sequential empty pages, a checkpoint file will be saved and the code will “break” out of the loop to account for a potential loss of connection/website issue without halting the code entirely. A print function was coded at every safety to provide the user an update on the errors/failures that have occurred. At the end of the while loop, the rows are added to the dataset using the “extend” method, the page counter is incremented, a checkpoint file is saved, and a short break was programmed to avoid overwhelming the server . Once the while loop concludes, all of the consolidated raw scraped data is saved as a final checkpoint file and the final json output file, which is “raw_scraped_data.json”; this json file will be loaded into the clean.py file for cleaning. 


clean.py – 

The primary purpose of clean.py is to take in the raw dataset produced by scrape.py (raw_scraped_data.json), execute two phases of cleaning, and produce two JSON output files: “applicant_data.json,” which will have all the cleaned data as well as the Program and University data fields cleaned by the local LLM and “applicant_data.json,” which is the final consolidated and cleaned dataset of 30,000 student entries. The first phase of cleaning is done through various implementation of REGEX, and the second phase is executed by the local LLM (app.py). Of note, the local LLM will only clean/format the Program and University data fields. The beginning of clean.py consists of code that will batch raw datasets in chunks to send through the local LLM without overwhelming it. The next section of code will focus on extracting the specific text for the desired data fields through REGEX, indexing, slicing, and matching. This process involves identified the end points of HTML text in which the desired text resides or pattern matching of the desired text utilizing REGEX.  
	 
Def chunked(): This function takes batches the raw dataset into chunks to moderate the volume of raw data sent through the local LLM. ChatGPT assisted with coding the way in which to take a full list of rows (lst) and break it into specified sized batches (size). Later when the chunk size is specified, this function will send the prescribed batches of Program and University data to the local LLM to eb cleaned. 
Def clean_whitespace(): This function utilizes the .sub REGEX and strip() methods to eliminate unnecessary and also standardize the spacing between words to facilitate parsing. ChatGPT assisted in coding the REGEX code to substitute any length of whitespace with just a single space to standardize spacing. 

Def clean_program_cell(): This function extracts and formats the text for the Program data from each student application. The code isolates the text of the academic program by eliminating a series of words that exist in the same row through the use of string indexing and slicing. The .find() method indexes the start and endpoint of the desired text and then strips unwanted whitespace to return the text of the Program. ChatGPT assisted in providing the concept of consolidating unwanted words into a list, and then utilizing indexing and slicing to eliminate them. 
def normalize_zero(): There were many data entries for the various GRE tests and GPAs that were listed as different forms of zero. ChatGPT assisted with the coding of turning the zeroes into strings and then correlating them to a score/GPA not being provided.  

def extract_notes: In the URLs in each of the student applications, there is a data field titled “Notes,” which are will be titled “comments” for the cleaned data set. This function extracts the notes text by first using the match variable to employ REGEX to find the word “Notes.” The .search() method targets the data that exists between the word “Notes” and “Timeline,” extracts it, eliminates unnecessary whitespace and standardizes spacing. ChatGPT assisted in the coding of the REGEX to do this. 

def extract_decision(): In a similar fashion, this function employs the REGEX method .search() to target and extract the text between the word “Decision” and “Notification” to extract the decision text. Same formatting procedure applies. In a similar fashion, ChatGPT helped code the REGEX to extract the decision text. 

def extract_notification_date(), def extract_degree_type(), extract_country_origin(), def extract_undergrad_gpa, def extract_gre_general, def extract_gre_verbal(), extract_gre_aw, and def extract_term_year follow a similar pattern of using the .search() REGEX method to target start and end points, extract the desired text field, and using REGEX to standardize the format of the text. The GRE scores are formatted through REGEX to have the same quantity of numbers and decimal places. ChatGPT assisted in the coding of the REGEX code to produce the desired appearance of all data fields. 
Def_load_data(): This function loads the raw dataset produced by scrape.py (applicant_data.json) def_save_data(): will write the cleaned data set that includes the llm-generated program and university text (llm_extend_applicant_data.json). 

Def_llm_post_rows() :This function prepares the payload by converting it to JSON and then into bytes to send it to the local LLM sever through a POST request. ChatGPT assisted in coding a retry process in the case the LLM fails or crashes and identifying the error to print to help troubleshoot. This code attempts the code send 5 times and idenfities potential errors it may encounter. 

Def clean_data(): This large block of code calls the functions described above to extract the desired text, format them to appear as they do in the sample assignment output, create a series of dictionaries to enable indexing, which will ultimately help package all the cleaned data into the format prescribed in the assignment sample out. Most importantly, ChatGPT provided the concept and helped code deduplication with regard to the process of identifying all Program-University pairs (many applications had the same pair), sending one unique Program and University pair to the local LLM, and then mapping it back to all the applications that had that pair. This significantly increases processing speed by avoiding redundant cleanings. Additionally, throughout the deduplication code, ChatGPT helped code several print outputs to demonstrate the amount of deduplication was occurring. The first portion of clean_data() focuses on cleaning the program-university pairs through the local LLM. 

First, the clean_whitespace function was called to eliminate redundance whitespace and standardize it with a single space. Then the focus is to prepare the Program and University data to be sent to the llm and ensure that only one “unique” pair was sent instead of duplicates (deduplication); ChatGPT assisted in the coding of this. For the deduplication process, once the program-university pair was created, they were established as keys and their corresponding values were the rows in which the pair exists. Then the program-university pair was sent to the LLM to be cleaned one time and then mapped back to every original application that contained the pair. Reference comments in code for detailed breakdown. Ultimately, ChatGPT provided the concept of deduplication as a way to significantly decrease the amount of time it took for the local LLM to process all of the program-university pairs across 30,000 student applications. 

The next portion of this code batches the unique program-university pairs in chunks of 100 to send to the local LLM. Originally, the batch size was set to 300, which crashed the LLM at approximately 50% completion; a chunk size of 100 ran cleanly. After sending the batches to the local LLM, the cleaned outputs are consolidated in unique_results. Once all batches are cleaned, a lookup dictionary is created that pairs up the original program-university pairs with the LLM-cleaned program-university pairs using the same index. Then the lookup is applied to every row, which allows the LLM-cleaned program-university pair to be returned to all applications that have that specific program-university pair. 

The next portion of this code then extracts the required fields out of the “results_text_raw”, which is the data pulled from the URLs in each students’ application.  ChatGPT assisted in coding the various ways to format the desired field text to match the assignment outputs. For example, for the decision, the .title() function was used to format the word “accepted” to always appear with the first letter capitalized and the rest lower case.  Another example is the Application Status field, which contained the decision and notification date of the decision. The code pairs the decision and notification date data together properly to format it to appear as it does in the assignment sample output. 

The final portion of the code packages all the cleaned data in order to properly print out the two required JSON files, which are applicant_data.json (does not include LLM-generated Program and University data) and the llm_extend_applicant_data.json (which does).

Lastly, def main() executes the clean_data() function and generates the two required json files. 


main.py

The purpose of main.py is to orchestrate the execution of app.py, scrape.py, and clean.py in the correct order. Additionally, ChatGPT assisted in coding safety measures to ensure that each script ran in its entirety and produced the correct output before moving onto the next script. Lastly, main.py prints a series of updates since this program was ran throughout the night; this provides confirmation that the program was successful or what errors occurred to facilitate troubleshooting.
 
ChatGPT assisted in the import section of the code. The os, sys, and subprocess libraries are required to run multiple python scripts, especially if they exist in different folders (i.e. app.py exists in the llm_hosting folder inside the module_2 folder). 

There were several issues with the CPU crashing to to high processing volume. ChatGPT provided the code in def_start_llm_server() to optimize computer settings to reduces crashes and overwhelming the CPU. 
def wait_for_llm() ensures that the LLM successfully executes and runs before starting scrape.py. ChatGPT assisted in coding the retry mechanism in the case that the LLM server fails to run. 
Def run_scrape() executes scrape.py and pauses main.py until it successfully finishes and produces the expected JSON file of raw data. Def wait_for_file() checks to ensure that the JSON file of raw data (raw_scraped_data.JSON) is generated. If it does not exists, it prints the error that is thrown for reference. 

def json_sanity_check() opens the generated JSON file and validates it as the proper format dataset to prevent issues further down the program pipeline. 

def main(): ensures that all python scripts are executed in order and calls the functions described above to ensure there are no issues between python script executions. ChatGPT assisted in coding to ensure that the python scripts execute from their respective file locations. 


  
Known Bugs: GitHub history. There was a lapse of 48 hours in my commit/push history due to having issues with PyCharm committing/pushing. I did not know to add the llm_hosting files and large raw data files into gitignore and PyCharm would not push due to file size. I eventually added all large folders in gitignore and only committed/pushed the python files while I continued to work. Once programming was complete, I committed and pushed the final python files as well as the applicant_data.json and llm_extend_applicant_data.json to my GitHub. Also, a batch side of 300 caused the LLM server and/or clean.py to crash at about 50% completion. Reducing too 100 enabled the code to execute successfully. Approximate run time for the whole program was 6 hours. 


Citations: ChatGPT was utilized throughout the programming process to learn concepts to incorporate into the code and to assist with the coding itself. Specific references are covered in the Approach section. 
